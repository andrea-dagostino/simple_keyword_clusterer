{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "import re\r\n",
    "import nltk\r\n",
    "from nltk.corpus import stopwords\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "from sklearn import cluster\r\n",
    "from sklearn import decomposition\r\n",
    "from sklearn import manifold\r\n",
    "from sklearn.metrics import silhouette_score\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "from tqdm import tqdm\r\n",
    "\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "STOPWORDS = list(set(stopwords.words('english')))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "to_remove = [\"junior\", \"senior\", \"stage\", \"sr\", \"entwickler\", \"time\", \"remote\"]\r\n",
    "STOPWORDS.extend(to_remove)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def sanitize_text(text: str, remove_stopwords: bool) -> str:\r\n",
    "    \"\"\"This utility function sanitizes a string by:\r\n",
    "    - removing links\r\n",
    "    - removing special characters\r\n",
    "    - removing numbers\r\n",
    "    - removing stopwords\r\n",
    "    - transforming in lowercase\r\n",
    "    - removing excessive whitespaces\r\n",
    "\r\n",
    "    Args:\r\n",
    "        text (str): the input text you want to clean\r\n",
    "        remove_stopwords (bool): whether or not to remove stopwords\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        str: the cleaned text\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    # remove links\r\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\r\n",
    "    # remove special chars and numbers\r\n",
    "    text = re.sub(\"[^A-Za-z]+\", \" \", text)\r\n",
    "    # remove stopwords\r\n",
    "    if remove_stopwords:\r\n",
    "        # 1. tokenize\r\n",
    "        tokens = nltk.word_tokenize(text)\r\n",
    "        # 2. check if stopword\r\n",
    "        tokens = [w for w in tokens if not w.lower() in STOPWORDS]\r\n",
    "        # 3. join back together\r\n",
    "        text = \" \".join(tokens)\r\n",
    "    # return text in lower case and stripped of whitespaces\r\n",
    "    text = text.lower().strip()\r\n",
    "    return text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "with open(\"./marketing.txt\", \"r\", encoding=\"utf-8\") as f:\r\n",
    "    jobs = f.read().splitlines()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "def normalize_role(text):\r\n",
    "    roles_to_normalize = [(\"front end\", \"frontend\"), (\"back end\", \"backend\")]\r\n",
    "    for wrong, right in roles_to_normalize:\r\n",
    "        if wrong in text:\r\n",
    "            return right + \" \" + text[len(wrong) + 1:]\r\n",
    "        else:\r\n",
    "            return text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "cleaned_jobs = [sanitize_text(job, remove_stopwords=True) for job in jobs]\r\n",
    "cleaned_roles = [normalize_role(job) for job in cleaned_jobs]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "def find_number_of_clusters(data, centers):\r\n",
    "    # A list holds the silhouette coefficients for each k\r\n",
    "    silhouette_coefficients = []\r\n",
    "\r\n",
    "    # Notice you start at 2 clusters for silhouette coefficient\r\n",
    "    for k in range(2, 20):\r\n",
    "        score = silhouette_score(data, centers)\r\n",
    "        silhouette_coefficients.append(score)\r\n",
    "    \r\n",
    "    return silhouette_coefficients.index(max(silhouette_coefficients))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "def find_elbow(clusterer):\r\n",
    "    sse = []\r\n",
    "    for i in range(2, 20):\r\n",
    "        sse.append(clusterer.inertia_)\r\n",
    "    kl = KneeLocator(range(2, 20), sse, curve=\"convex\", direction=\"decreasing\")\r\n",
    "    return kl.elbow\r\n",
    "     "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "def find_elbow1(X):\r\n",
    "    range_n_clusters = range(2, 50)         # clusters range you want to select\r\n",
    "    best_clusters = 0                       # best cluster number which you will get\r\n",
    "    previous_silh_avg = 0.0\r\n",
    "\r\n",
    "    for n_clusters in tqdm(range_n_clusters, desc=\"Finding clusters...\"):\r\n",
    "        clusterer = cluster.KMeans(n_clusters=n_clusters)\r\n",
    "        cluster_labels = clusterer.fit_predict(X)\r\n",
    "        silhouette_avg = silhouette_score(X, cluster_labels)\r\n",
    "        if silhouette_avg > previous_silh_avg:\r\n",
    "            previous_silh_avg = silhouette_avg\r\n",
    "            best_clusters = n_clusters\r\n",
    "    return best_clusters"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "def run(vectorizer, cleaned_jobs):\r\n",
    "    X = vectorizer.fit_transform(cleaned_jobs).toarray()\r\n",
    "    voc = vectorizer.get_feature_names()\r\n",
    "\r\n",
    "    opt_n_clusters = find_elbow1(X)\r\n",
    "    print(f\"Found number of optimal clusters: {opt_n_clusters}\")\r\n",
    "\r\n",
    "    clusterer = cluster.KMeans(n_clusters=opt_n_clusters, random_state=42).fit(X)\r\n",
    "    clusters = clusterer.predict(X)\r\n",
    "\r\n",
    "    decomposer = decomposition.PCA(n_components=2, random_state=42)\r\n",
    "    reduced = decomposer.fit_transform(X)\r\n",
    "\r\n",
    "\r\n",
    "    centers = decomposer.transform(clusterer.cluster_centers_)\r\n",
    "\r\n",
    "    \r\n",
    "    component_1, component_2 = reduced[:, 0], reduced[:, 1]\r\n",
    "    df = pd.concat([pd.Series(component_1, name=\"PCA1\"), \\\r\n",
    "        pd.Series(component_2, name=\"PCA2\"), \\\r\n",
    "            pd.Series(clusters, name=\"cluster\")], axis=1)\r\n",
    "\r\n",
    "    plt.figure(figsize=(10, 8))\r\n",
    "    \r\n",
    "    # print(\"Top terms per cluster:\")\r\n",
    "    order_centroids = clusterer.cluster_centers_.argsort()[:, ::-1]\r\n",
    "    kws = []\r\n",
    "    for i, (_x, _y) in enumerate(centers):\r\n",
    "        # print(\"Cluster %d:\" % i),\r\n",
    "        for ind in order_centroids[i, :1]:\r\n",
    "            # print(' %s' % voc[ind])\r\n",
    "            kws.append(voc[ind])\r\n",
    "            # plt.text(_x, _y, f\"{i}_{voc[ind]}\", horizontalalignment='left', size='medium', color='black')\r\n",
    "    \r\n",
    "    df['cluster'] = df['cluster'].map({key:value for (key, value) in enumerate(kws)})\r\n",
    "\r\n",
    "    sns.scatterplot(data=df, x=\"PCA1\", y=\"PCA2\", hue=\"cluster\", palette=\"coolwarm\", s=100)\r\n",
    "    plt.title(f\"Normalized job roles, num. clusters: {opt_n_clusters}\", fontsize=18)\r\n",
    "    plt.legend(fontsize=15, title=\"Clusters\", fancybox=True)\r\n",
    "    plt.show()\r\n",
    "\r\n",
    "    df['input'] = cleaned_roles\r\n",
    "\r\n",
    "    return df\r\n",
    "\r\n",
    "df = run(TfidfVectorizer(ngram_range=(2, 2), smooth_idf=True, sublinear_tf=True), cleaned_roles)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Finding clusters...:   2%|‚ñè         | 1/48 [00:00<00:18,  2.60it/s]"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "# todo: remove \"full time\" and other temporal indicators\r\n",
    "# find ideal number of clusters automatically"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "df.to_csv(\"test.csv\", encoding=\"utf-8\", index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('job_title_extractor': conda)"
  },
  "interpreter": {
   "hash": "d48a49fb488f0e8b56e288a264d5a3e803ecb2a2d18faa53e9b0e1d5e746a188"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}