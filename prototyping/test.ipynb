{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import cluster\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn import model_selection\n",
    "from sklearn import compose\n",
    "from sklearn import pipeline\n",
    "from sklearn import base\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "STOPWORDS = list(set(stopwords.words('italian')))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "to_remove = [\"junior\", \"senior\", \"stage\", \"sr\", \"entwickler\", \"time\", \"remote\"]\n",
    "STOPWORDS.extend(to_remove)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def sanitize_text(text: str, remove_stopwords: bool) -> str:\n",
    "    \"\"\"This utility function sanitizes a string by:\n",
    "    - removing links\n",
    "    - removing special characters\n",
    "    - removing numbers\n",
    "    - removing stopwords\n",
    "    - transforming in lowercase\n",
    "    - removing excessive whitespaces\n",
    "\n",
    "    Args:\n",
    "        text (str): the input text you want to clean\n",
    "        remove_stopwords (bool): whether or not to remove stopwords\n",
    "\n",
    "    Returns:\n",
    "        str: the cleaned text\n",
    "    \"\"\"\n",
    "\n",
    "    # remove links\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # remove special chars and numbers\n",
    "    text = re.sub(\"[^A-Za-z]+\", \" \", text)\n",
    "    # remove stopwords\n",
    "    if remove_stopwords:\n",
    "        # 1. tokenize\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        # 2. check if stopword\n",
    "        tokens = [w for w in tokens if not w.lower() in STOPWORDS]\n",
    "        # 3. join back together\n",
    "        text = \" \".join(tokens)\n",
    "    # return text in lower case and stripped of whitespaces\n",
    "    text = text.lower().strip()\n",
    "    return text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "with open(\"./jobs.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    jobs = f.read().splitlines()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def normalize_role(text):\n",
    "    roles_to_normalize = [(\"front end\", \"frontend\"), (\"back end\", \"backend\")]\n",
    "    for wrong, right in roles_to_normalize:\n",
    "        if wrong in text:\n",
    "            return right + \" \" + text[len(wrong) + 1:]\n",
    "        else:\n",
    "            return text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "kws = pd.read_excel(\"kwset.xlsx\")\n",
    "kws = list(kws.keyword)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "cleaned_jobs = [sanitize_text(job, remove_stopwords=True) for job in jobs]\n",
    "cleaned_roles = [normalize_role(job) for job in cleaned_jobs]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "def find_elbow(X):\n",
    "    range_n_clusters = range(2, 20)         # clusters range you want to select\n",
    "    best_clusters = 0                       # best cluster number which you will get\n",
    "    previous_silh_avg = 0.0\n",
    "\n",
    "    for n_clusters in tqdm(range_n_clusters, desc=\"Finding clusters...\"):\n",
    "        clusterer = cluster.KMeans(n_clusters=n_clusters)\n",
    "        cluster_labels = clusterer.fit_predict(X)\n",
    "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "        if silhouette_avg > previous_silh_avg:\n",
    "            previous_silh_avg = silhouette_avg\n",
    "            best_clusters = n_clusters\n",
    "    return (silhouette_avg, best_clusters)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "def run(vectorizer, cleaned_jobs):\n",
    "    X = vectorizer.fit_transform(cleaned_jobs).toarray()\n",
    "    voc = vectorizer.get_feature_names()\n",
    "\n",
    "    sil_score, opt_n_clusters = find_elbow(X)\n",
    "    # print(f\"Found number of optimal clusters: {opt_n_clusters}\")\n",
    "\n",
    "    clusterer = cluster.KMeans(n_clusters=opt_n_clusters, random_state=42).fit(X)\n",
    "    clusters = clusterer.predict(X)\n",
    "\n",
    "    decomposer = decomposition.PCA(n_components=2, random_state=42)\n",
    "    reduced = decomposer.fit_transform(X)\n",
    "\n",
    "    centers = decomposer.transform(clusterer.cluster_centers_)\n",
    "\n",
    "    component_1, component_2 = reduced[:, 0], reduced[:, 1]\n",
    "    df = pd.concat([pd.Series(component_1, name=\"PCA1\"), \\\n",
    "        pd.Series(component_2, name=\"PCA2\"), \\\n",
    "            pd.Series(clusters, name=\"cluster\")], axis=1)\n",
    "\n",
    "    \n",
    "    # print(\"Top terms per cluster:\")\n",
    "    order_centroids = clusterer.cluster_centers_.argsort()[:, ::-1]\n",
    "    kws = []\n",
    "    for i, (_x, _y) in enumerate(centers):\n",
    "        # print(\"Cluster %d:\" % i),\n",
    "        for ind in order_centroids[i, :1]:\n",
    "            # print(' %s' % voc[ind])\n",
    "            kws.append(voc[ind])\n",
    "            # plt.text(_x, _y, f\"{i}_{voc[ind]}\", horizontalalignment='left', size='medium', color='black')\n",
    "    \n",
    "    df['cluster'] = df['cluster'].map({key:value for (key, value) in enumerate(kws)})\n",
    "\n",
    "    res = {\"silhouette\": sil_score, \"clusters\": opt_n_clusters}\n",
    "\n",
    "    return res\n",
    "\n",
    "    # plt.figure(figsize=(10, 8))\n",
    "    # sns.scatterplot(data=df, x=\"PCA1\", y=\"PCA2\", hue=\"cluster\", palette=\"coolwarm\", s=100)\n",
    "    # plt.title(f\"Num roles {len(cleaned_roles)}, num. clusters: {opt_n_clusters}\", fontsize=18)\n",
    "    # plt.legend(fontsize=10, title=\"Clusters\")\n",
    "    # plt.show()\n",
    "\n",
    "    # df['input'] = cleaned_roles\n",
    "\n",
    "    # return df\n",
    "\n",
    "df = run(TfidfVectorizer(ngram_range=(2, 2), max_features=50, smooth_idf=True, sublinear_tf=True), cleaned_roles)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Finding clusters...: 100%|██████████| 18/18 [00:01<00:00, 13.74it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found number of optimal clusters: 19\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "class ItemCleaner(base.BaseEstimator, base.TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def clean_text(self, text, remove_stopwords):\n",
    "        # remove links\n",
    "        text = re.sub(r\"http\\S+\", \"\", text)\n",
    "        # remove special chars and numbers\n",
    "        text = re.sub(\"[^A-Za-z]+\", \" \", text)\n",
    "        # remove stopwords\n",
    "        if remove_stopwords:\n",
    "            # 1. tokenize\n",
    "            tokens = nltk.word_tokenize(text)\n",
    "            # 2. check if stopword\n",
    "            tokens = [w for w in tokens if not w.lower() in STOPWORDS]\n",
    "            # 3. join back together\n",
    "            text = \" \".join(tokens)\n",
    "        # return text in lower case and stripped of whitespaces\n",
    "        text = text.lower().strip()\n",
    "        return text\n",
    "    \n",
    "    def normalize_role(self, text):\n",
    "        roles_to_normalize = [(\"front end\", \"frontend\"), (\"back end\", \"backend\")]\n",
    "        for wrong, right in roles_to_normalize:\n",
    "            if wrong in text:\n",
    "                return right + \" \" + text[len(wrong) + 1:]\n",
    "            else:\n",
    "                return text\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        for i in range(X.shape[0]):\n",
    "            X[i] = self.clean_text(X[i])\n",
    "            X[i] = self.normalize_role(X[i])\n",
    "        return X \n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "main = pd.DataFrame(jobs, columns=[\"item\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "cleaning_pipeline = pipeline.Pipeline(steps=[\n",
    "    (\"cleaner\", ItemCleaner)\n",
    "    ])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "vec = TfidfVectorizer()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "preprocessor = compose.ColumnTransformer(transformers=[(\"cleaning\", cleaning_pipeline, \"item\")])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "pipeline = pipeline.Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('vectorizer', TfidfVectorizer),\n",
    "    ('kmeans', cluster.KMeans(random_state=42)),\n",
    "    ('pca', decomposition.PCA(n_components=2))\n",
    "])"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "closing parenthesis ']' does not match opening parenthesis '(' on line 5 (2218434313.py, line 6)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/1n/ql5h_9qj50zf33rf5npmlnzr0000gn/T/ipykernel_21224/2218434313.py\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    ])\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m closing parenthesis ']' does not match opening parenthesis '(' on line 5\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "parameters = {\n",
    "    'max_df': (0.25, 0.5, 0.75),\n",
    "    'ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'smooth_idf': [True, False],\n",
    "    'sublinear_tf': [True, False],\n",
    "    'max_features': [200, 500, 1000, 2000, 5000]\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# todo: remove \"full time\" and other temporal indicators\n",
    "# find ideal number of clusters automatically \n",
    "# todo: do gridsearch, try these metrics:\n",
    "#@ https://stackoverflow.com/questions/44066264/how-to-choose-parameters-in-tfidfvectorizer-in-sklearn-during-unsupervised-clust\n",
    "# Calinski-Harabasz Index\n",
    "# Davies-Bouldin Index\n",
    "# https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "vec = TfidfVectorizer(ngram_range=(2, 3), smooth_idf=True, sublinear_tf=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "m = vec.fit_transform(cleaned_roles).todense()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "m"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('job_title': conda)"
  },
  "interpreter": {
   "hash": "b42d9328231c38327d9edad7b6f34dc92d16fd75e6bc0d6d36c0de17b31b5583"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}