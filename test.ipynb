{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "import re\r\n",
    "import nltk\r\n",
    "from nltk.corpus import stopwords\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "from sklearn.cluster import KMeans\r\n",
    "from sklearn.decomposition import PCA, NMF\r\n",
    "from sklearn.manifold import TSNE\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "STOPWORDS = list(set(stopwords.words('english')))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "def sanitize_text(text: str, remove_stopwords: bool) -> str:\r\n",
    "    \"\"\"This utility function sanitizes a string by:\r\n",
    "    - removing links\r\n",
    "    - removing special characters\r\n",
    "    - removing numbers\r\n",
    "    - removing stopwords\r\n",
    "    - transforming in lowercase\r\n",
    "    - removing excessive whitespaces\r\n",
    "\r\n",
    "    Args:\r\n",
    "        text (str): the input text you want to clean\r\n",
    "        remove_stopwords (bool): whether or not to remove stopwords\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        str: the cleaned text\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    # remove links\r\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\r\n",
    "    # remove special chars and numbers\r\n",
    "    text = re.sub(\"[^A-Za-z]+\", \" \", text)\r\n",
    "    # remove stopwords\r\n",
    "    if remove_stopwords:\r\n",
    "        # 1. tokenize\r\n",
    "        tokens = nltk.word_tokenize(text)\r\n",
    "        # 2. check if stopword\r\n",
    "        tokens = [w for w in tokens if not w.lower() in STOPWORDS]\r\n",
    "        # 3. join back together\r\n",
    "        text = \" \".join(tokens)\r\n",
    "    # return text in lower case and stripped of whitespaces\r\n",
    "    text = text.lower().strip()\r\n",
    "    return text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "with open(\"./jobs.txt\", \"r\", encoding=\"utf-8\") as f:\r\n",
    "    jobs = f.read().splitlines()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "to_remove = [\"junior\", \"senior\", \"stage\", \"sr\"]\r\n",
    "STOPWORDS.extend(to_remove)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "def normalize_role(text):\r\n",
    "    roles_to_normalize = [(\"front end\", \"frontend\"), (\"back end\", \"backend\")]\r\n",
    "    for wrong, right in roles_to_normalize:\r\n",
    "        if wrong in text:\r\n",
    "            return right + \" \" + text[len(wrong) + 1:]\r\n",
    "        else:\r\n",
    "            return text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "cleaned_jobs = [sanitize_text(job, remove_stopwords=True) for job in devs]\r\n",
    "cleaned_roles = [normalize_role(job) for job in cleaned_jobs]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "source": [
    "def run(vectorizer, cleaned_jobs, n_clusters):\r\n",
    "    X = vectorizer.fit_transform(cleaned_jobs).toarray()\r\n",
    "    voc = vectorizer.get_feature_names()\r\n",
    "    kmeans = KMeans(n_clusters=n_clusters).fit(X)\r\n",
    "    clusters = kmeans.predict(X)\r\n",
    "    pca = PCA(n_components=2)\r\n",
    "    reduced = pca.fit_transform(X)\r\n",
    "\r\n",
    "    centers = pca.transform(kmeans.cluster_centers_)\r\n",
    "    \r\n",
    "    pca_1, pca_2 = reduced[:, 0], reduced[:, 1]\r\n",
    "    df = pd.concat([pd.Series(pca_1, name=\"PCA1\"), \\\r\n",
    "        pd.Series(pca_2, name=\"PCA2\"), \\\r\n",
    "            pd.Series(clusters, name=\"cluster\")], axis=1)\r\n",
    "\r\n",
    "    plt.figure(figsize=(10, 8))\r\n",
    "    \r\n",
    "\r\n",
    "    \r\n",
    "    print(\"Top terms per cluster:\")\r\n",
    "    order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\r\n",
    "    kws = []\r\n",
    "    for i, (_x, _y) in enumerate(centers):\r\n",
    "        print(\"Cluster %d:\" % i),\r\n",
    "        for ind in order_centroids[i, :1]:\r\n",
    "            print(' %s' % voc[ind])\r\n",
    "            kws.append(voc[ind])\r\n",
    "            # plt.text(_x, _y, f\"{i}_{voc[ind]}\", horizontalalignment='left', size='medium', color='black')\r\n",
    "    \r\n",
    "    df['cluster'] = df['cluster'].map({key:value for (key, value) in enumerate(kws)})\r\n",
    "\r\n",
    "    sns.scatterplot(data=df, x=\"PCA1\", y=\"PCA2\", hue=\"cluster\", palette=\"coolwarm\")\r\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "source": [
    "run(TfidfVectorizer(ngram_range=(2, 3)), cleaned_roles, 15)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('job_title_extractor': conda)"
  },
  "interpreter": {
   "hash": "d48a49fb488f0e8b56e288a264d5a3e803ecb2a2d18faa53e9b0e1d5e746a188"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}